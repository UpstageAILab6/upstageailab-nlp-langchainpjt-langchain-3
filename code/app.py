import streamlit as st
import pandas as pd
from config import ConfigLoader
from loader import DocumentLoader
from vectorstore import VectorStoreManager
from llm import GovPolicyLLM
from prompt import GovPolicyPrompt, MarkdownFormatter
from gov_policy_qa import GovPolicyQA
from chat_history import ChatHistory
from logger import Logger
from grounding_checker import VectorstoreGroundingChecker
from langchain_openai import OpenAIEmbeddings

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ì •ë¶€ ì •ì±… ì±—ë´‡", layout="wide")
st.title("ğŸ‡°ğŸ‡· ì •ë¶€ ì§€ì› ì •ì±… ì§ˆë¬¸ ì±—ë´‡")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = ChatHistory()

if "qa_system" not in st.session_state:
    config = ConfigLoader(project_name="GovPolicyQA")
    config.load()

    st.session_state.logger = Logger()

    doc_loader = DocumentLoader(filepath="data/serviceDetail_all.csv")
    documents = doc_loader.load_documents()

    embeddings = OpenAIEmbeddings()
    vs_manager = VectorStoreManager(embeddings=embeddings)
    vectorstore = vs_manager.load_or_create(documents, path="code/faiss_index_v2")

    llm = GovPolicyLLM(model_name="gpt-4o", temperature=0).get_llm()
    prompt = GovPolicyPrompt().get_prompt()
    formatter = MarkdownFormatter()

    st.session_state.qa_system = GovPolicyQA(vectorstore, embeddings, llm, prompt, formatter)
    st.session_state.grounding_checker = VectorstoreGroundingChecker(vectorstore, embeddings)

# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
user_input = st.text_input("ì •ë¶€ ì§€ì› ì •ì±…ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”", "")

if user_input:
    st.session_state.logger.log_user_input(user_input)
    st.session_state.chat_history.add_user_message(user_input)

    with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
        answer = st.session_state.qa_system.run(user_input)

    # ğŸ” ê·¸ë¼ìš´ë“œì²´í‚¹ ì‹¤í–‰
    grounding_results = st.session_state.grounding_checker.run(answer)
    grounded_count = sum(r["grounded"] for r in grounding_results)
    total_count = len(grounding_results)

    # ğŸ“Œ ë‹µë³€ ì¶œë ¥
    st.markdown("---")
    st.subheader("ğŸ“Œ ë‹µë³€")
    st.markdown(answer, unsafe_allow_html=True)

    # âœ… ê·¸ë¼ìš´ë“œì²´í‚¹ í•œ ì¤„ ìš”ì•½
    if total_count > 0:
        ratio = grounded_count / total_count * 100
        st.markdown(
            f"ğŸ§  **[ê·¸ë¼ìš´ë“œì²´í‚¹ ìš”ì•½]** ì´ {total_count}ê°œ ë¬¸ì¥ ì¤‘ {grounded_count}ê°œ ë¬¸ì¥ì´ ë¬¸ì„œ ê¸°ë°˜ ì •ë³´ì™€ ì¼ì¹˜í•©ë‹ˆë‹¤. "
            f"(**ì •í™•ë„: {ratio:.1f}%**)"
        )
    else:
        st.markdown("âš ï¸ **[ê·¸ë¼ìš´ë“œì²´í‚¹ ìš”ì•½]** ë¬¸ì¥ ê¸°ë°˜ ê²€ì¦ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ğŸ§¾ ë¬¸ì¥-ê·¼ê±° ë§¤í•‘ í…Œì´ë¸”
    st.markdown("---")
    st.subheader("ğŸ§¾ ë¬¸ì¥ë³„ ìœ ì‚¬ë„ ë° ê·¼ê±° ë¬¸ì„œ ë§¤í•‘ ê²°ê³¼")
    if grounding_results:
        df = pd.DataFrame(grounding_results)
        df_display = df[["sentence", "matched_context", "similarity", "grounded"]]
        df_display["similarity"] = df_display["similarity"].apply(lambda x: round(x, 3))
        df_display["grounded"] = df_display["grounded"].apply(lambda x: "âœ…" if x else "âŒ")
        st.dataframe(df_display, use_container_width=True)
    else:
        st.info("í‘œì‹œí•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")

    st.session_state.logger.log_assistant_response(answer)
    st.session_state.chat_history.add_assistant_message(answer)

# ğŸ’¬ ëŒ€í™” íˆìŠ¤í† ë¦¬
if st.session_state.chat_history.get_history():
    st.markdown("---")
    st.subheader("ğŸ’¬ ëŒ€í™” ê¸°ë¡")
    for message in st.session_state.chat_history.get_history():
        role = message["role"]
        content = message["content"]
        if role == "user":
            st.markdown(f"**ğŸ‘¤ ì‚¬ìš©ì:** {content}")
        elif role == "assistant":
            st.markdown(f"**ğŸ¤– ì±—ë´‡:**\n\n{content}", unsafe_allow_html=True)