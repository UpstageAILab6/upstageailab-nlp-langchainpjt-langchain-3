{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\duffp\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.12-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langchain-core\n",
      "  Downloading langchain_core-0.3.50-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting langchain-text-splitters\n",
      "  Downloading langchain_text_splitters-0.3.7-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.10.0-cp312-cp312-win_amd64.whl.metadata (4.5 kB)\n",
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.25.5-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting openai<2.0.0,>=1.68.2 (from langchain-openai)\n",
      "  Using cached openai-1.70.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
      "  Downloading tiktoken-0.9.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting langsmith<0.4,>=0.1.125 (from langchain-core)\n",
      "  Downloading langsmith-0.3.24-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from langchain-core) (8.2.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from langchain-core) (6.0.1)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from langchain-core) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from langchain-core) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from langchain-core) (2.10.6)\n",
      "Collecting langchain<1.0.0,>=0.3.21 (from langchain-community)\n",
      "  Downloading langchain-0.3.22-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from langchain-community) (2.0.34)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from langchain-community) (3.10.5)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from langchain-community) (2.6.1)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.11.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core) (0.27.0)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.125->langchain-core)\n",
      "  Downloading orjson-3.10.16-cp312-cp312-win_amd64.whl.metadata (42 kB)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.68.2->langchain-openai)\n",
      "  Downloading jiter-0.9.0-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.66.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.68.2->langchain-openai) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Downloading langchain_openai-0.3.12-py3-none-any.whl (61 kB)\n",
      "Downloading langchain_core-0.3.50-py3-none-any.whl (423 kB)\n",
      "Downloading langchain_community-0.3.20-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.5/2.5 MB 48.1 MB/s eta 0:00:00\n",
      "Downloading langchain_text_splitters-0.3.7-py3-none-any.whl (32 kB)\n",
      "Downloading faiss_cpu-1.10.0-cp312-cp312-win_amd64.whl (13.7 MB)\n",
      "   ---------------------------------------- 0.0/13.7 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 7.6/13.7 MB 39.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.7/13.7 MB 35.8 MB/s eta 0:00:00\n",
      "Downloading pymupdf-1.25.5-cp39-abi3-win_amd64.whl (16.6 MB)\n",
      "   ---------------------------------------- 0.0/16.6 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 9.2/16.6 MB 43.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 15.7/16.6 MB 38.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.6/16.6 MB 36.0 MB/s eta 0:00:00\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading langchain-0.3.22-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 24.2 MB/s eta 0:00:00\n",
      "Downloading langsmith-0.3.24-py3-none-any.whl (352 kB)\n",
      "Using cached openai-1.70.0-py3-none-any.whl (599 kB)\n",
      "Downloading tiktoken-0.9.0-cp312-cp312-win_amd64.whl (894 kB)\n",
      "   ---------------------------------------- 0.0/894.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 894.9/894.9 kB 20.4 MB/s eta 0:00:00\n",
      "Downloading jiter-0.9.0-cp312-cp312-win_amd64.whl (207 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading orjson-3.10.16-cp312-cp312-win_amd64.whl (133 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Installing collected packages: typing-inspect, pymupdf, orjson, marshmallow, jiter, httpx-sse, faiss-cpu, tiktoken, dataclasses-json, openai, langsmith, langchain-core, langchain-text-splitters, langchain-openai, langchain, langchain-community\n",
      "Successfully installed dataclasses-json-0.6.7 faiss-cpu-1.10.0 httpx-sse-0.4.0 jiter-0.9.0 langchain-0.3.22 langchain-community-0.3.20 langchain-core-0.3.50 langchain-openai-0.3.12 langchain-text-splitters-0.3.7 langsmith-0.3.24 marshmallow-3.26.1 openai-1.70.0 orjson-3.10.16 pymupdf-1.25.5 tiktoken-0.9.0 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv langchain-openai langchain-core langchain-community langchain-text-splitters faiss-cpu pymupdf\n",
    "%pip install streamlit\n",
    "%pip install tiktoken\n",
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenAI í‚¤ ë¡œë“œë¨: True\n",
      "âœ… LangSmith í‚¤ ë¡œë“œë¨: True\n",
      "í˜„ì¬ LangSmith í”„ë¡œì íŠ¸: Test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import Client\n",
    "from langchain_core.tracers import LangChainTracer\n",
    "\n",
    "# .env íŒŒì¼ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# âœ… í™˜ê²½ ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸° ìƒíƒœ í™•ì¸\n",
    "print(\"âœ… OpenAI í‚¤ ë¡œë“œë¨:\", os.getenv(\"OPENAI_API_KEY\") is not None)\n",
    "print(\"âœ… LangSmith í‚¤ ë¡œë“œë¨:\", os.getenv(\"LANGSMITH_API_KEY\") is not None)\n",
    "\n",
    "# LangSmith í™˜ê²½ ì„¤ì • (ë™ì  ì„¤ì •)\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")  # .envì—ì„œ ë¶ˆëŸ¬ì˜´\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = os.getenv(\"LANGSMITH_ENDPOINT\") or \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Test\"  # ì›í•˜ëŠ” í”„ë¡œì íŠ¸ ì´ë¦„\n",
    "\n",
    "# LangSmith í´ë¼ì´ì–¸íŠ¸ ì§ì ‘ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŒ\n",
    "client = Client()\n",
    "print(\"í˜„ì¬ LangSmith í”„ë¡œì íŠ¸:\", os.environ[\"LANGCHAIN_PROJECT\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… íŒŒì¼ ê²½ë¡œ í™•ì¸ ì™„ë£Œ: C:\\Users\\duffp\\RAG\\upstageailab-nlp-langchainpjt-langchain-3\\data\\gov24_serviceList_all.json\n",
      "ğŸ“¦ JSON ë¡œë“œ ì™„ë£Œ: í•­ëª© ìˆ˜ 10245ê°œ\n"
     ]
    }
   ],
   "source": [
    "# 1. ì ˆëŒ€ê²½ë¡œ ì§€ì •\n",
    "absolute_path = r\"C:\\Users\\duffp\\RAG\\upstageailab-nlp-langchainpjt-langchain-3\\data\\gov24_serviceList_all.json\"\n",
    "\n",
    "# 2. íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n",
    "if os.path.exists(absolute_path):\n",
    "    print(\"âœ… íŒŒì¼ ê²½ë¡œ í™•ì¸ ì™„ë£Œ:\", absolute_path)\n",
    "else:\n",
    "    print(\"âŒ ê²½ë¡œì— íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 3. JSON ë¡œë“œ í•¨ìˆ˜ì— ì§ì ‘ ê²½ë¡œ ë„˜ê¸°ê¸°\n",
    "def load_json_from_absolute_path(file_path: str):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"ğŸ“¦ JSON ë¡œë“œ ì™„ë£Œ: í•­ëª© ìˆ˜ {len(data)}ê°œ\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}\")\n",
    "        return []\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "data = load_json_from_absolute_path(absolute_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain ë¬¸ì„œ ë³€í™˜ ì™„ë£Œ: 10245ê°œ\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = []\n",
    "for item in data:\n",
    "    content = f\"\"\"\n",
    "ì„œë¹„ìŠ¤ëª…: {item.get('ì„œë¹„ìŠ¤ëª…')}\n",
    "ì„œë¹„ìŠ¤ëª©ì : {item.get('ì„œë¹„ìŠ¤ëª©ì ìš”ì•½')}\n",
    "ì§€ì›ëŒ€ìƒ: {item.get('ì§€ì›ëŒ€ìƒ')}\n",
    "ì§€ì›ë‚´ìš©: {item.get('ì§€ì›ë‚´ìš©')}\n",
    "ì‹ ì²­ë°©ë²•: {item.get('ì‹ ì²­ë°©ë²•')}\n",
    "ì‹ ì²­ê¸°í•œ: {item.get('ì‹ ì²­ê¸°í•œ')}\n",
    "ì„ ì •ê¸°ì¤€: {item.get('ì„ ì •ê¸°ì¤€')}\n",
    "ì„œë¹„ìŠ¤ë¶„ì•¼: {item.get('ì„œë¹„ìŠ¤ë¶„ì•¼')}\n",
    "ì†Œê´€ê¸°ê´€: {item.get('ì†Œê´€ê¸°ê´€ëª…')}\n",
    "ë¬¸ì˜ì „í™”: {item.get('ì „í™”ë¬¸ì˜')}\n",
    "ìƒì„¸ì¡°íšŒURL: {item.get('ìƒì„¸ì¡°íšŒURL')}\n",
    "\"\"\"\n",
    "    documents.append(Document(page_content=content.strip(), metadata={\"ì„œë¹„ìŠ¤ID\": item.get(\"ì„œë¹„ìŠ¤ID\")}))\n",
    "\n",
    "print(f\"LangChain ë¬¸ì„œ ë³€í™˜ ì™„ë£Œ: {len(documents)}ê°œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¶„í• ëœ ë¬¸ì„œ ìˆ˜: 11721\n",
      "ì²« ì²­í¬ ë‚´ìš©:\n",
      " ì„œë¹„ìŠ¤ëª…: ìœ ì•„í•™ë¹„ (ëˆ„ë¦¬ê³¼ì •) ì§€ì›\n",
      "ì„œë¹„ìŠ¤ëª©ì : ìœ ì¹˜ì›ì— ë‹¤ë‹ˆëŠ” ë§Œ 3~5ì„¸ ì•„ë™ì—ê²Œ ìœ ì•„í•™ë¹„, ë°©ê³¼í›„ê³¼ì •ë¹„ ë“± ì§€ì›\n",
      "ì§€ì›ëŒ€ìƒ: â—‹ ì§€ì›ëŒ€ìƒ : êµ­ê³µë¦½ ë° ì‚¬ë¦½ìœ ì¹˜ì›ì— ë‹¤ë‹ˆëŠ” 3~5ì„¸ ìœ ì•„\n",
      "  \n",
      "   - '22ë…„ 1~2ì›”ìƒìœ¼ë¡œ ìœ ì¹˜ì› ì…í•™ì„ í¬ë§í•˜ì—¬ 3ì„¸ë°˜ì— ì·¨ì›í•œ ìœ ì•„ë„ ì§€ì› ëŒ€ìƒ\n",
      "   -  ì·¨í•™ëŒ€ìƒ ì•„ë™('18.1.1~12.31.ì¶œìƒ)ì´ ì·¨í•™ì„ ìœ ì˜ˆí•˜ëŠ” ê²½ìš°, ìœ ì˜ˆí•œ 1ë…„ì— í•œí•˜ì—¬ 5ì„¸ ìœ ì•„ ë¬´ìƒêµìœ¡ë¹„ ì§€ì›(ì·¨í•™ìœ ì˜ˆ í†µì§€ì„œ ì œì¶œ)\n",
      "   â€» ë‹¨, ì§€ì›ê¸°ê°„ì€ 3ë…„ì„ ì´ˆê³¼í•  ìˆ˜ ì—†ìŒ.\n",
      "\n",
      "\n",
      "\n",
      "â—‹ ì¶”ê°€ì§€ì› : ì €ì†Œë“ì¸µ ìœ ì•„(ìœ ì•„í•™ë¹„ ì§€ì› ëŒ€ìƒ ìê²©ì´ ìˆê³ , ì‚¬ë¦½ìœ ì¹˜ì›ì— ë‹¤ë‹ˆëŠ” ë²•ì •ì €ì†Œë“ì¸µ(ê¸°ì´ˆìƒí™œìˆ˜ê¸‰ì, ì°¨ìƒìœ„ê³„ì¸µ, í•œë¶€ëª¨ ê°€ì •) ìœ ì•„)\n",
      "\n",
      "â—‹  ì•„ë˜ì˜ ê²½ìš° ì§€ì›ëŒ€ìƒì—ì„œ ì œì™¸\n",
      "   -  ëŒ€í•œë¯¼êµ­ êµ­ì ì„ ê°€ì§€ì§€ ì•Šì€ ìœ ì•„(ë‚œë¯¼ ë° ã€Œì¬í•œì™¸êµ­ì¸ ì²˜ìš° ê¸°ë³¸ë²•ã€ì— ë”°ë¼ ë²•ë¬´ë¶€ì¥ê´€ì´ ì¸ì •í•œ 'íŠ¹ë³„ê¸°ì—¬ì ë“±'ì€ ì˜ˆì™¸ì ìœ¼ë¡œ ì¸ì •)\n",
      "   - ê°€ì • ì–‘ìœ¡ìˆ˜ë‹¹ ë° ì–´ë¦°ì´ì§‘ ë³´ìœ¡ë£Œë¥¼ ì§€ì›\n"
     ]
    }
   ],
   "source": [
    "split_documents = text_splitter.split_documents(documents)\n",
    "print(f\"ë¶„í• ëœ ë¬¸ì„œ ìˆ˜: {len(split_documents)}\")\n",
    "print(\"ì²« ì²­í¬ ë‚´ìš©:\\n\", split_documents[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ì²­í¬ 1 ---\n",
      "ì„œë¹„ìŠ¤ëª…: ìœ ì•„í•™ë¹„ (ëˆ„ë¦¬ê³¼ì •) ì§€ì›\n",
      "ì„œë¹„ìŠ¤ëª©ì : ìœ ì¹˜ì›ì— ë‹¤ë‹ˆëŠ” ë§Œ 3~5ì„¸ ì•„ë™ì—ê²Œ ìœ ì•„í•™ë¹„, ë°©ê³¼í›„ê³¼ì •ë¹„ ë“± ì§€ì›\n",
      "ì§€ì›ëŒ€ìƒ: â—‹ ì§€ì›ëŒ€ìƒ : êµ­ê³µë¦½ ë° ì‚¬ë¦½ìœ ì¹˜ì›ì— ë‹¤ë‹ˆëŠ” 3~5ì„¸ ìœ ì•„\n",
      "  \n",
      "   - '22ë…„ 1~2ì›”ìƒìœ¼ë¡œ ìœ ì¹˜ì› ì…í•™ì„ í¬ë§í•˜ì—¬ 3ì„¸ë°˜ì— ì·¨ì›í•œ ìœ ì•„ë„ ì§€ì› ëŒ€ìƒ\n",
      "   -  ì·¨í•™ëŒ€ìƒ ì•„ë™('18.1.1~12.31.ì¶œìƒ)ì´ ì·¨í•™ì„ ìœ ì˜ˆí•˜ëŠ” ê²½ìš°, ìœ ì˜ˆí•œ 1ë…„ì— í•œí•˜ì—¬ 5ì„¸ ìœ ì•„ ë¬´ìƒêµìœ¡ë¹„ ì§€ì›(ì·¨í•™ìœ ì˜ˆ í†µì§€ì„œ ì œì¶œ)\n",
      "   â€» ë‹¨, ì§€ì›ê¸°ê°„ì€ 3ë…„ì„ ì´ˆê³¼í•  ìˆ˜ ì—†ìŒ.\n",
      "\n",
      "\n",
      "\n",
      "â—‹ ì¶”ê°€ì§€ì› : ì €ì†Œë“ì¸µ ìœ ì•„(ìœ ì•„í•™ë¹„ ì§€ì› ëŒ€ìƒ ìê²©ì´ ìˆê³ , ì‚¬ë¦½ìœ ì¹˜ì›ì— ë‹¤ë‹ˆëŠ” ë²•ì •ì €ì†Œë“ì¸µ(ê¸°ì´ˆìƒí™œìˆ˜ê¸‰ì, ì°¨ìƒìœ„ê³„ì¸µ, í•œë¶€ëª¨ ê°€ì •) ìœ ì•„)\n",
      "\n",
      "â—‹  ì•„ë˜ì˜ ê²½ìš° ì§€ì›ëŒ€ìƒì—ì„œ ì œì™¸\n",
      "   -  ëŒ€í•œë¯¼êµ­ êµ­ì ì„ ê°€ì§€ì§€ ì•Šì€ ìœ ì•„(ë‚œë¯¼ ë° ã€Œì¬í•œì™¸êµ­ì¸ ì²˜ìš° ê¸°ë³¸ë²•ã€ì— ë”°ë¼ ë²•ë¬´ë¶€ì¥ê´€ì´ ì¸ì •í•œ 'íŠ¹ë³„ê¸°ì—¬ì ë“±'ì€ ì˜ˆì™¸ì ìœ¼ë¡œ ì¸ì •)\n",
      "   - ê°€ì • ì–‘ìœ¡ìˆ˜ë‹¹ ë° ì–´ë¦°ì´ì§‘ ë³´ìœ¡ë£Œë¥¼ ì§€ì› ë°›ê³  ìˆëŠ” ìœ ì•„\n",
      "   -  ìœ ì¹˜ì› ì´ìš©ì‹œê°„ì— ì•„ì´ëŒë´„ì„œë¹„ìŠ¤ ë“±ê³¼ ì¤‘ë³µì§€ì› ë¶ˆê°€\n",
      "   - í•´ì™¸ ì²´ë¥˜ ê¸°ê°„ì´ 31ì¼ì§¸ ë˜ëŠ” ë‚  ìœ ì•„í•™ë¹„ ì§€ì›ìê²© ì¤‘ì§€\n",
      "\n",
      "â—‹  ìê²© ì¤‘ì§€ í›„ ìœ ì•„í•™ë¹„ë¥¼ ë‹¤ì‹œ ì§€ì›ë°›ê¸° ìœ„í•´ì„œëŠ” ì¬ì‹ ì²­ í•„ìš”, ì‹ ì²­ ëˆ„ë½ìœ¼ë¡œ ë°œìƒë˜ëŠ” ì§€ì›ê¸ˆì€ ì†Œê¸‰ì§€ì› ë˜ì§€ ì•ŠìŒ.\n",
      "ì§€ì›ë‚´ìš©: â—‹ 3~5ì„¸ì— ëŒ€í•´ êµìœ¡ë¹„ë¥¼ ì§€ê¸‰í•©ë‹ˆë‹¤.\n",
      "  - êµ­ê³µë¦½ 100,000ì›, ì‚¬ë¦½ 280,000ì›\n",
      "\n",
      "â—‹ 3~5ì„¸ì— ëŒ€í•´ ë°©ê³¼í›„ê³¼ì •ë¹„ë¥¼ ì§€ê¸‰í•©ë‹ˆë‹¤.\n",
      "   - êµ­ê³µë¦½ 50,000ì›, ì‚¬ë¦½ 70,000ì›\n",
      "\n",
      "--- ì²­í¬ 2 ---\n",
      "- êµ­ê³µë¦½ 50,000ì›, ì‚¬ë¦½ 70,000ì›\n",
      "\n",
      "â—‹ ì‚¬ë¦½ìœ ì¹˜ì›ì„ ë‹¤ë‹ˆëŠ” ë²•ì •ì €ì†Œë“ì¸µ ìœ ì•„ì—ê²Œ ì €ì†Œë“ì¸µ ìœ ì•„í•™ë¹„ë¥¼ ì¶”ê°€ ì§€ê¸‰í•©ë‹ˆë‹¤.\n",
      "   - ì‚¬ë¦½ 200,000ì›\n",
      "ì‹ ì²­ë°©ë²•: ê¸°íƒ€ ì˜¨ë¼ì¸ì‹ ì²­||ë°©ë¬¸ì‹ ì²­\n",
      "ì‹ ì²­ê¸°í•œ: ìƒì‹œì‹ ì²­\n",
      "ì„ ì •ê¸°ì¤€: â€» 2025. 3. 1~2026.2.28. ê¹Œì§€ ì ìš©\n",
      "\n",
      "â—‹ ì§€ì›ëŒ€ìƒ : êµ­ê³µë¦½ìœ ì¹˜ì› ë° ì‚¬ë¦½ìœ ì¹˜ì›ì— ë‹¤ë‹ˆëŠ”  ë§Œ 3~5ì„¸ ì•„ë™\n",
      "       5ì„¸  '19.1.1.~'19.12.31.\n",
      "       4ì„¸  '20.1.1.~'20.12.31.\n",
      "       3ì„¸  '21.1.1.~'22.2.28.\n",
      "\n",
      "\n",
      "â—‹ ì‹ ì²­ì¸ : ì•„ë™ì˜ ë³´í˜¸ì\n",
      "\n",
      "â—‹ ì‹ ì²­ì¥ì†Œ : ì˜¨ë¼ì¸ ì‹ ì²­ (ë³µì§€ë¡œ ë³µì§€ì‚¬ì—…ì„œë¹„ìŠ¤ (www.bokjiro.go.kr) ) ë° ìë©´ë™ ì£¼ë¯¼ì„¼í„°(ì•„ë™ ì£¼ë¯¼ë“±ë¡ ì£¼ì†Œì§€)\n",
      "\n",
      " - ì£¼ì˜: ì˜¨ë¼ì¸ ì‹ ì²­ì€ ë¶€ëª¨ë§Œ ê°€ëŠ¥\n",
      "   â€» ë¶€ëª¨ ì´ì™¸ì˜ ë³´í˜¸ìì¸ ê²½ìš°(ìë…€ì˜ ì¹œê¶Œì ë˜ëŠ”  í›„ê²¬ì¸ ë³´í˜¸ì, ì¡°ë¶€ëª¨, ì‚¬íšŒë³µì§€ì‹œì„¤ì¥ ë“±) ë“± ë‹´ë‹¹ê³µë¬´ì›ì˜ í™•ì¸ì´ í•„ìš”í•œ ê²½ìš°ëŠ” ì˜¨ë¼ì¸ìœ¼ë¡œ ì‹ ì²­í•˜ì‹¤ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ë¶ˆí¸í•˜ì‹œë”ë¼ë„ ìë©´ë™ ì£¼ë¯¼ì„¼í„°(ì£¼ì†Œì§€ ì‹œêµ°êµ¬)ì—ì„œ ì‹ ì²­í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.\n",
      "ì„œë¹„ìŠ¤ë¶„ì•¼: ë³´ìœ¡Â·êµìœ¡\n",
      "ì†Œê´€ê¸°ê´€: êµìœ¡ë¶€\n",
      "ë¬¸ì˜ì „í™”: êµìœ¡ë¶€/02-6222-6060||0079ì—ë“€ì½œ/1544-0079-5-1\n",
      "ìƒì„¸ì¡°íšŒURL: https://www.gov.kr/portal/rcvfvrSvc/dtlEx/000000465790\n",
      "\n",
      "--- ì²­í¬ 3 ---\n",
      "ì„œë¹„ìŠ¤ëª…: ê·¼ë¡œÂ·ìë…€ì¥ë ¤ê¸ˆ\n",
      "ì„œë¹„ìŠ¤ëª©ì : ì†Œë“ê³¼ ì¬ì‚°ì´ ì ì€ ê·¼ë¡œì†Œë“ìì—ê²Œ ê·¼ë¡œì¥ë ¤ê¸ˆì„, ìë…€ê°€ ìˆì„ ê²½ìš° ìë…€ì¥ë ¤ê¸ˆ ì§€ê¸‰\n",
      "ì§€ì›ëŒ€ìƒ: â—‹ ì‹ ì²­ìš”ê±´ì„ ëª¨ë‘ ì¶©ì¡±í•˜ëŠ” ê·¼ë¡œì†Œë“, ì‚¬ì—…ì†Œë“ ë˜ëŠ” ì¢…êµì¸ì†Œë“ì´ ìˆëŠ” ê°€êµ¬ë¡œì¨ ì‹ ì²­ê¸°ê°„ ë‚´ ì‹ ì²­í•˜ëŠ” ê²½ìš° ê·¼ë¡œì†Œë“, ì‚¬ì—…ì†Œë“ ë˜ëŠ” ì¢…êµì¸ì†Œë“ì— ë”°ë¼ ì‚°ì •í•œ ê·¼ë¡œì¥ë ¤ê¸ˆê³¼ ë¶€ì–‘ìë…€ìˆ˜ì— ë”°ë¼ ì‚°ì •í•œ ìë…€ì¥ë ¤ê¸ˆì„ ì§€ê¸‰\n",
      "ì§€ì›ë‚´ìš©: â—‹ ì „ë…„ë„ ì—°ê°„ ë¶€ë¶€í•©ì‚° ì´ ê¸‰ì—¬ì•¡ ë“±(ê·¼ë¡œì†Œë“, ì‚¬ì—…ì†Œë“ ë˜ëŠ” ì¢…êµì¸ì†Œë“ì˜ í•©ê³„)ì— ë”°ë¼\n",
      " - ê·¼ë¡œì¥ë ¤ê¸ˆì€\n",
      "  ã† ë‹¨ë…ê°€êµ¬ ìµœëŒ€ 165ë§Œ ì›\n",
      "  ã† í™‘ë²Œì´ ê°€êµ¬ ìµœëŒ€ 285ë§Œ ì›\n",
      "  ã† ë§ë²Œì´ ê°€êµ¬ ìµœëŒ€ 330ë§Œ ì› ì§€ê¸‰\n",
      " - ìë…€ ì¥ë ¤ê¸ˆì€\n",
      "  ã† ë‹¨ë…ê°€êµ¬ í•´ë‹¹ ì—†ìŒ\n",
      "  ã† í™‘ë²Œì´ ê°€êµ¬ ë¶€ì–‘ìë…€ 1ëª… ë‹¹ ìµœëŒ€ 100ë§Œ ì›\n",
      "  ã† ë§ë²Œì´ ê°€êµ¬ ë¶€ì–‘ìë…€ 1ëª… ë‹¹ ìµœëŒ€ 100ë§Œ ì› ì§€ê¸‰\n",
      "\n",
      "* ìì„¸í•œ ì‚°ì •ì‹ì€ í™ˆíƒìŠ¤(www.hometax.go.kr)ì—ì„œ í™•ì¸ ë°”ëë‹ˆë‹¤\n",
      "ì‹ ì²­ë°©ë²•: ê¸°íƒ€ ì˜¨ë¼ì¸ì‹ ì²­\n",
      "ì‹ ì²­ê¸°í•œ: â—‹ ì •ê¸°ì‹ ì²­ : 5.1.~5.31.â—‹ ë°˜ê¸°ì‹ ì²­ - ìƒë°˜ê¸°ë¶„ ì‹ ì²­ : 9.1.~9.15. - í•˜ë°˜ê¸°ë¶„ ì‹ ì²­ : 3.1.~3.15.\n",
      "ì„ ì •ê¸°ì¤€: â—‹ ì•„ë˜ ìš”ê±´ì„ ëª¨ë‘ ì¶©ì¡±í•˜ëŠ” ê·¼ë¡œì†Œë“, ì‚¬ì—…ì†Œë“ ë˜ëŠ” ì¢…êµì¸ì†Œë“ì´ ìˆëŠ” ê°€êµ¬\n",
      "\n",
      "- ì†Œë“ìš”ê±´ : ì „ë…„ë„ ë¶€ë¶€í•©ì‚° ì—°ê°„ ì´ì†Œë“ì´ ê°€êµ¬ ìœ í˜•ì— ë”°ë¼ ì •í•œ ì´ ì†Œë“ê¸°ì¤€ê¸ˆì•¡ ë¯¸ë§Œì¼ ê²ƒ\n",
      "  (ê·¼ë¡œì¥ë ¤ê¸ˆ)\n",
      "  ã† ë‹¨ë…ê°€êµ¬ : 2,200ë§Œ ì› ë¯¸ë§Œ\n",
      "  ã† í™‘ë²Œì´ ê°€êµ¬ : 3,200ë§Œ ì› ë¯¸ë§Œ\n",
      "  ã† ë§ë²Œì´ ê°€êµ¬ : 3,800ë§Œ ì› ë¯¸ë§Œ\n",
      "  (ìë…€ì¥ë ¤ê¸ˆ)\n",
      "  ã† 7,000ë§Œ ì› ë¯¸ë§Œ\n",
      " - ì¬ì‚°ìš”ê±´\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(split_documents[:3]):\n",
    "    print(f\"\\n--- ì²­í¬ {i+1} ---\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAIEmbeddingsëŠ” OPENAI_API_KEYë¥¼ ìë™ìœ¼ë¡œ .envì—ì„œ ë¶ˆëŸ¬ì˜´\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§® ì´ í† í° ìˆ˜: 4,816,550\n",
      "ğŸ’¸ ì˜ˆìƒ ì„ë² ë”© ë¹„ìš©: $0.096331 USD\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def estimate_embedding_cost(docs, model=\"text-embedding-3-small\", price_per_1k=0.00002):\n",
    "    \"\"\"\n",
    "    ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•œ ì´ í† í° ìˆ˜ ë° ì˜ˆìƒ ë¹„ìš© ê³„ì‚°\n",
    "\n",
    "    Args:\n",
    "        docs: LangChain Document ë¦¬ìŠ¤íŠ¸\n",
    "        model: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ëª… (ê¸°ë³¸: text-embedding-3-small)\n",
    "        price_per_1k: 1K í† í°ë‹¹ ë¹„ìš© (ë‹¬ëŸ¬)\n",
    "\n",
    "    Returns:\n",
    "        total_tokens, estimated_cost\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")  # ëŒ€ë¶€ë¶„ ë™ì¼ í† í¬ë‚˜ì´ì € ì‚¬ìš©\n",
    "    total_tokens = sum(len(encoding.encode(doc.page_content)) for doc in docs)\n",
    "    estimated_cost = (total_tokens / 1000) * price_per_1k\n",
    "    return total_tokens, estimated_cost\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "tokens, cost = estimate_embedding_cost(split_documents)\n",
    "print(f\"ğŸ§® ì´ í† í° ìˆ˜: {tokens:,}\")\n",
    "print(f\"ğŸ’¸ ì˜ˆìƒ ì„ë² ë”© ë¹„ìš©: ${cost:.6f} USD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ˆê¸°í™”\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\n",
    "\n",
    "# ë””ìŠ¤í¬ì— ì €ì¥\n",
    "vectorstore.save_local(\"faiss_store\")\n",
    "\n",
    "print(\"FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# í† í° ì‚¬ìš© ì•ˆ í•˜ê³  ê¸°ì¡´ ë²¡í„°ë¥¼ ë¡œë“œí•¨\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.load_local(\"faiss_store\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_sim = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "retriever_mmr = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 10, \"lambda_mult\": 0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"\"\"ë„ˆëŠ” ë³µì§€ í˜œíƒì„ ì¶”ì²œí•´ì£¼ëŠ” ì±—ë´‡ì´ì•¼.\n",
    "ì•„ë˜ëŠ” ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í˜œíƒ ë¬¸ì„œë“¤ì´ì•¼.\"\n",
    "\n",
    "# Context:\n",
    "{context}\n",
    "\n",
    "# Question:\n",
    "{question}\n",
    "\n",
    "# Answer:\n",
    "- ê´€ë ¨ëœ ë³µì§€ í˜œíƒì„ ìì—°ìŠ¤ëŸ½ê³  ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ì¤˜\n",
    "- ëŒ€ìƒ ì¡°ê±´ê³¼ ì‹ ì²­ ë°©ë²•ë„ ê°„ë‹¨íˆ ì•Œë ¤ì¤˜\n",
    "- í˜œíƒì´ ì—¬ëŸ¬ ê°œë©´ ìˆœì„œëŒ€ë¡œ ì •ë¦¬í•´ì¤˜\n",
    "- í•œê¸€ë¡œ, ë¶€ë“œëŸ½ê³  ê³µì†í•œ ë§íˆ¬ë¡œ ì‘ì„±í•´ì¤˜\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith íŠ¸ë ˆì´ì‹±ì€ .env ì„¤ì •ë§Œìœ¼ë¡œ ìë™ í™œì„±í™”ë¨\n",
    "# LANGSMITH_TRACING=true ì„¤ì • ì‹œ ì‹¤í–‰ ë¡œê·¸ë¥¼ LangSmithì—ì„œ í™•ì¸ ê°€ëŠ¥\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_sim = (\n",
    "    {\"context\": retriever_sim, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain_mmr = (\n",
    "    {\"context\": retriever_mmr, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 17:32:00.805 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\duffp\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-04-05 17:32:00.806 Session state does not function when running a script without `streamlit run`\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "\n",
    "# ì œëª©\n",
    "st.title(\"í˜œíƒ ì¶”ì²œ ì‹œìŠ¤í…œ: Similarity vs MMR\")\n",
    "\n",
    "# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥ë°›ê¸°\n",
    "question = st.text_input(\"ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”\", placeholder=\"ì˜ˆ: ê²½ê¸°ë„ì— ê±°ì£¼í•˜ëŠ” 29ì‚´ ë‚¨ìì¸ë° ë‚´ê°€ ë°›ì„ ìˆ˜ ìˆëŠ” í˜œíƒì´ ìˆì„ê¹Œ?\")\n",
    "\n",
    "# ë²„íŠ¼ í´ë¦­ ì‹œ ì‹¤í–‰\n",
    "if st.button(\"í˜œíƒ ì¶”ì²œ ë°›ê¸°\"):\n",
    "    if question:\n",
    "        # ìœ ì‚¬ë„ ë°©ì‹ ì‘ë‹µ\n",
    "        response_sim = chain_sim.invoke(question)\n",
    "        # MMR ë°©ì‹ ì‘ë‹µ\n",
    "        response_mmr = chain_mmr.invoke(question)\n",
    "\n",
    "        # ì¶œë ¥\n",
    "        st.subheader(\"ğŸ”¹ Similarity ë°©ì‹ ì‘ë‹µ\")\n",
    "        st.write(response_sim)\n",
    "\n",
    "        st.subheader(\"ğŸ”¸ MMR ë°©ì‹ ì‘ë‹µ\")\n",
    "        st.write(response_mmr)\n",
    "    else:\n",
    "        st.warning(\"ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook lanhchain.ipynb to script\n",
      "[NbConvertApp] Writing 5807 bytes to lanhchain.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script lanhchain.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
