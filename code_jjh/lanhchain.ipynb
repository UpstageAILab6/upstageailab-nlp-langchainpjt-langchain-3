{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\duffp\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.12-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langchain-core\n",
      "  Downloading langchain_core-0.3.50-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting langchain-text-splitters\n",
      "  Downloading langchain_text_splitters-0.3.7-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.10.0-cp312-cp312-win_amd64.whl.metadata (4.5 kB)\n",
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.25.5-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting openai<2.0.0,>=1.68.2 (from langchain-openai)\n",
      "  Using cached openai-1.70.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
      "  Downloading tiktoken-0.9.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting langsmith<0.4,>=0.1.125 (from langchain-core)\n",
      "  Downloading langsmith-0.3.24-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from langchain-core) (8.2.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from langchain-core) (6.0.1)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from langchain-core) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from langchain-core) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from langchain-core) (2.10.6)\n",
      "Collecting langchain<1.0.0,>=0.3.21 (from langchain-community)\n",
      "  Downloading langchain-0.3.22-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from langchain-community) (2.0.34)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from langchain-community) (3.10.5)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from langchain-community) (2.6.1)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.11.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core) (0.27.0)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.125->langchain-core)\n",
      "  Downloading orjson-3.10.16-cp312-cp312-win_amd64.whl.metadata (42 kB)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.68.2->langchain-openai)\n",
      "  Downloading jiter-0.9.0-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.66.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.68.2->langchain-openai) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\duffp\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Downloading langchain_openai-0.3.12-py3-none-any.whl (61 kB)\n",
      "Downloading langchain_core-0.3.50-py3-none-any.whl (423 kB)\n",
      "Downloading langchain_community-0.3.20-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.5/2.5 MB 48.1 MB/s eta 0:00:00\n",
      "Downloading langchain_text_splitters-0.3.7-py3-none-any.whl (32 kB)\n",
      "Downloading faiss_cpu-1.10.0-cp312-cp312-win_amd64.whl (13.7 MB)\n",
      "   ---------------------------------------- 0.0/13.7 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 7.6/13.7 MB 39.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.7/13.7 MB 35.8 MB/s eta 0:00:00\n",
      "Downloading pymupdf-1.25.5-cp39-abi3-win_amd64.whl (16.6 MB)\n",
      "   ---------------------------------------- 0.0/16.6 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 9.2/16.6 MB 43.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 15.7/16.6 MB 38.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.6/16.6 MB 36.0 MB/s eta 0:00:00\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading langchain-0.3.22-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 24.2 MB/s eta 0:00:00\n",
      "Downloading langsmith-0.3.24-py3-none-any.whl (352 kB)\n",
      "Using cached openai-1.70.0-py3-none-any.whl (599 kB)\n",
      "Downloading tiktoken-0.9.0-cp312-cp312-win_amd64.whl (894 kB)\n",
      "   ---------------------------------------- 0.0/894.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 894.9/894.9 kB 20.4 MB/s eta 0:00:00\n",
      "Downloading jiter-0.9.0-cp312-cp312-win_amd64.whl (207 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading orjson-3.10.16-cp312-cp312-win_amd64.whl (133 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Installing collected packages: typing-inspect, pymupdf, orjson, marshmallow, jiter, httpx-sse, faiss-cpu, tiktoken, dataclasses-json, openai, langsmith, langchain-core, langchain-text-splitters, langchain-openai, langchain, langchain-community\n",
      "Successfully installed dataclasses-json-0.6.7 faiss-cpu-1.10.0 httpx-sse-0.4.0 jiter-0.9.0 langchain-0.3.22 langchain-community-0.3.20 langchain-core-0.3.50 langchain-openai-0.3.12 langchain-text-splitters-0.3.7 langsmith-0.3.24 marshmallow-3.26.1 openai-1.70.0 orjson-3.10.16 pymupdf-1.25.5 tiktoken-0.9.0 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv langchain-openai langchain-core langchain-community langchain-text-splitters faiss-cpu pymupdf\n",
    "%pip install streamlit\n",
    "%pip install tiktoken\n",
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenAI 키 로드됨: True\n",
      "✅ LangSmith 키 로드됨: True\n",
      "현재 LangSmith 프로젝트: Test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import Client\n",
    "from langchain_core.tracers import LangChainTracer\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "# ✅ 환경 변수 불러오기 상태 확인\n",
    "print(\"✅ OpenAI 키 로드됨:\", os.getenv(\"OPENAI_API_KEY\") is not None)\n",
    "print(\"✅ LangSmith 키 로드됨:\", os.getenv(\"LANGSMITH_API_KEY\") is not None)\n",
    "\n",
    "# LangSmith 환경 설정 (동적 설정)\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")  # .env에서 불러옴\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = os.getenv(\"LANGSMITH_ENDPOINT\") or \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Test\"  # 원하는 프로젝트 이름\n",
    "\n",
    "# LangSmith 클라이언트 직접 사용할 수도 있음\n",
    "client = Client()\n",
    "print(\"현재 LangSmith 프로젝트:\", os.environ[\"LANGCHAIN_PROJECT\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 파일 경로 확인 완료: C:\\Users\\duffp\\RAG\\upstageailab-nlp-langchainpjt-langchain-3\\data\\gov24_serviceList_all.json\n",
      "📦 JSON 로드 완료: 항목 수 10245개\n"
     ]
    }
   ],
   "source": [
    "# 1. 절대경로 지정\n",
    "absolute_path = r\"C:\\Users\\duffp\\RAG\\upstageailab-nlp-langchainpjt-langchain-3\\data\\gov24_serviceList_all.json\"\n",
    "\n",
    "# 2. 파일 존재 여부 확인\n",
    "if os.path.exists(absolute_path):\n",
    "    print(\"✅ 파일 경로 확인 완료:\", absolute_path)\n",
    "else:\n",
    "    print(\"❌ 경로에 파일이 존재하지 않습니다.\")\n",
    "\n",
    "# 3. JSON 로드 함수에 직접 경로 넘기기\n",
    "def load_json_from_absolute_path(file_path: str):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"📦 JSON 로드 완료: 항목 수 {len(data)}개\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 파일 로드 오류: {e}\")\n",
    "        return []\n",
    "\n",
    "# 사용 예시\n",
    "data = load_json_from_absolute_path(absolute_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain 문서 변환 완료: 10245개\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = []\n",
    "for item in data:\n",
    "    content = f\"\"\"\n",
    "서비스명: {item.get('서비스명')}\n",
    "서비스목적: {item.get('서비스목적요약')}\n",
    "지원대상: {item.get('지원대상')}\n",
    "지원내용: {item.get('지원내용')}\n",
    "신청방법: {item.get('신청방법')}\n",
    "신청기한: {item.get('신청기한')}\n",
    "선정기준: {item.get('선정기준')}\n",
    "서비스분야: {item.get('서비스분야')}\n",
    "소관기관: {item.get('소관기관명')}\n",
    "문의전화: {item.get('전화문의')}\n",
    "상세조회URL: {item.get('상세조회URL')}\n",
    "\"\"\"\n",
    "    documents.append(Document(page_content=content.strip(), metadata={\"서비스ID\": item.get(\"서비스ID\")}))\n",
    "\n",
    "print(f\"LangChain 문서 변환 완료: {len(documents)}개\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분할된 문서 수: 11721\n",
      "첫 청크 내용:\n",
      " 서비스명: 유아학비 (누리과정) 지원\n",
      "서비스목적: 유치원에 다니는 만 3~5세 아동에게 유아학비, 방과후과정비 등 지원\n",
      "지원대상: ○ 지원대상 : 국공립 및 사립유치원에 다니는 3~5세 유아\n",
      "  \n",
      "   - '22년 1~2월생으로 유치원 입학을 희망하여 3세반에 취원한 유아도 지원 대상\n",
      "   -  취학대상 아동('18.1.1~12.31.출생)이 취학을 유예하는 경우, 유예한 1년에 한하여 5세 유아 무상교육비 지원(취학유예 통지서 제출)\n",
      "   ※ 단, 지원기간은 3년을 초과할 수 없음.\n",
      "\n",
      "\n",
      "\n",
      "○ 추가지원 : 저소득층 유아(유아학비 지원 대상 자격이 있고, 사립유치원에 다니는 법정저소득층(기초생활수급자, 차상위계층, 한부모 가정) 유아)\n",
      "\n",
      "○  아래의 경우 지원대상에서 제외\n",
      "   -  대한민국 국적을 가지지 않은 유아(난민 및 「재한외국인 처우 기본법」에 따라 법무부장관이 인정한 '특별기여자 등'은 예외적으로 인정)\n",
      "   - 가정 양육수당 및 어린이집 보육료를 지원\n"
     ]
    }
   ],
   "source": [
    "split_documents = text_splitter.split_documents(documents)\n",
    "print(f\"분할된 문서 수: {len(split_documents)}\")\n",
    "print(\"첫 청크 내용:\\n\", split_documents[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 청크 1 ---\n",
      "서비스명: 유아학비 (누리과정) 지원\n",
      "서비스목적: 유치원에 다니는 만 3~5세 아동에게 유아학비, 방과후과정비 등 지원\n",
      "지원대상: ○ 지원대상 : 국공립 및 사립유치원에 다니는 3~5세 유아\n",
      "  \n",
      "   - '22년 1~2월생으로 유치원 입학을 희망하여 3세반에 취원한 유아도 지원 대상\n",
      "   -  취학대상 아동('18.1.1~12.31.출생)이 취학을 유예하는 경우, 유예한 1년에 한하여 5세 유아 무상교육비 지원(취학유예 통지서 제출)\n",
      "   ※ 단, 지원기간은 3년을 초과할 수 없음.\n",
      "\n",
      "\n",
      "\n",
      "○ 추가지원 : 저소득층 유아(유아학비 지원 대상 자격이 있고, 사립유치원에 다니는 법정저소득층(기초생활수급자, 차상위계층, 한부모 가정) 유아)\n",
      "\n",
      "○  아래의 경우 지원대상에서 제외\n",
      "   -  대한민국 국적을 가지지 않은 유아(난민 및 「재한외국인 처우 기본법」에 따라 법무부장관이 인정한 '특별기여자 등'은 예외적으로 인정)\n",
      "   - 가정 양육수당 및 어린이집 보육료를 지원 받고 있는 유아\n",
      "   -  유치원 이용시간에 아이돌봄서비스 등과 중복지원 불가\n",
      "   - 해외 체류 기간이 31일째 되는 날 유아학비 지원자격 중지\n",
      "\n",
      "○  자격 중지 후 유아학비를 다시 지원받기 위해서는 재신청 필요, 신청 누락으로 발생되는 지원금은 소급지원 되지 않음.\n",
      "지원내용: ○ 3~5세에 대해 교육비를 지급합니다.\n",
      "  - 국공립 100,000원, 사립 280,000원\n",
      "\n",
      "○ 3~5세에 대해 방과후과정비를 지급합니다.\n",
      "   - 국공립 50,000원, 사립 70,000원\n",
      "\n",
      "--- 청크 2 ---\n",
      "- 국공립 50,000원, 사립 70,000원\n",
      "\n",
      "○ 사립유치원을 다니는 법정저소득층 유아에게 저소득층 유아학비를 추가 지급합니다.\n",
      "   - 사립 200,000원\n",
      "신청방법: 기타 온라인신청||방문신청\n",
      "신청기한: 상시신청\n",
      "선정기준: ※ 2025. 3. 1~2026.2.28. 까지 적용\n",
      "\n",
      "○ 지원대상 : 국공립유치원 및 사립유치원에 다니는  만 3~5세 아동\n",
      "       5세  '19.1.1.~'19.12.31.\n",
      "       4세  '20.1.1.~'20.12.31.\n",
      "       3세  '21.1.1.~'22.2.28.\n",
      "\n",
      "\n",
      "○ 신청인 : 아동의 보호자\n",
      "\n",
      "○ 신청장소 : 온라인 신청 (복지로 복지사업서비스 (www.bokjiro.go.kr) ) 및 읍면동 주민센터(아동 주민등록 주소지)\n",
      "\n",
      " - 주의: 온라인 신청은 부모만 가능\n",
      "   ※ 부모 이외의 보호자인 경우(자녀의 친권자 또는  후견인 보호자, 조부모, 사회복지시설장 등) 등 담당공무원의 확인이 필요한 경우는 온라인으로 신청하실수 없으므로 불편하시더라도 읍면동 주민센터(주소지 시군구)에서 신청하시기 바랍니다.\n",
      "서비스분야: 보육·교육\n",
      "소관기관: 교육부\n",
      "문의전화: 교육부/02-6222-6060||0079에듀콜/1544-0079-5-1\n",
      "상세조회URL: https://www.gov.kr/portal/rcvfvrSvc/dtlEx/000000465790\n",
      "\n",
      "--- 청크 3 ---\n",
      "서비스명: 근로·자녀장려금\n",
      "서비스목적: 소득과 재산이 적은 근로소득자에게 근로장려금을, 자녀가 있을 경우 자녀장려금 지급\n",
      "지원대상: ○ 신청요건을 모두 충족하는 근로소득, 사업소득 또는 종교인소득이 있는 가구로써 신청기간 내 신청하는 경우 근로소득, 사업소득 또는 종교인소득에 따라 산정한 근로장려금과 부양자녀수에 따라 산정한 자녀장려금을 지급\n",
      "지원내용: ○ 전년도 연간 부부합산 총 급여액 등(근로소득, 사업소득 또는 종교인소득의 합계)에 따라\n",
      " - 근로장려금은\n",
      "  ㆍ 단독가구 최대 165만 원\n",
      "  ㆍ 홑벌이 가구 최대 285만 원\n",
      "  ㆍ 맞벌이 가구 최대 330만 원 지급\n",
      " - 자녀 장려금은\n",
      "  ㆍ 단독가구 해당 없음\n",
      "  ㆍ 홑벌이 가구 부양자녀 1명 당 최대 100만 원\n",
      "  ㆍ 맞벌이 가구 부양자녀 1명 당 최대 100만 원 지급\n",
      "\n",
      "* 자세한 산정식은 홈택스(www.hometax.go.kr)에서 확인 바랍니다\n",
      "신청방법: 기타 온라인신청\n",
      "신청기한: ○ 정기신청 : 5.1.~5.31.○ 반기신청 - 상반기분 신청 : 9.1.~9.15. - 하반기분 신청 : 3.1.~3.15.\n",
      "선정기준: ○ 아래 요건을 모두 충족하는 근로소득, 사업소득 또는 종교인소득이 있는 가구\n",
      "\n",
      "- 소득요건 : 전년도 부부합산 연간 총소득이 가구 유형에 따라 정한 총 소득기준금액 미만일 것\n",
      "  (근로장려금)\n",
      "  ㆍ 단독가구 : 2,200만 원 미만\n",
      "  ㆍ 홑벌이 가구 : 3,200만 원 미만\n",
      "  ㆍ 맞벌이 가구 : 3,800만 원 미만\n",
      "  (자녀장려금)\n",
      "  ㆍ 7,000만 원 미만\n",
      " - 재산요건\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(split_documents[:3]):\n",
    "    print(f\"\\n--- 청크 {i+1} ---\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAIEmbeddings는 OPENAI_API_KEY를 자동으로 .env에서 불러옴\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧮 총 토큰 수: 4,816,550\n",
      "💸 예상 임베딩 비용: $0.096331 USD\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def estimate_embedding_cost(docs, model=\"text-embedding-3-small\", price_per_1k=0.00002):\n",
    "    \"\"\"\n",
    "    문서 리스트에 대한 총 토큰 수 및 예상 비용 계산\n",
    "\n",
    "    Args:\n",
    "        docs: LangChain Document 리스트\n",
    "        model: 사용할 임베딩 모델명 (기본: text-embedding-3-small)\n",
    "        price_per_1k: 1K 토큰당 비용 (달러)\n",
    "\n",
    "    Returns:\n",
    "        total_tokens, estimated_cost\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")  # 대부분 동일 토크나이저 사용\n",
    "    total_tokens = sum(len(encoding.encode(doc.page_content)) for doc in docs)\n",
    "    estimated_cost = (total_tokens / 1000) * price_per_1k\n",
    "    return total_tokens, estimated_cost\n",
    "\n",
    "# 사용 예시\n",
    "tokens, cost = estimate_embedding_cost(split_documents)\n",
    "print(f\"🧮 총 토큰 수: {tokens:,}\")\n",
    "print(f\"💸 예상 임베딩 비용: ${cost:.6f} USD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기화\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\n",
    "\n",
    "# 디스크에 저장\n",
    "vectorstore.save_local(\"faiss_store\")\n",
    "\n",
    "print(\"FAISS 벡터스토어 생성 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# 토큰 사용 안 하고 기존 벡터를 로드함\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.load_local(\"faiss_store\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_sim = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "retriever_mmr = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 10, \"lambda_mult\": 0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"\"\"너는 복지 혜택을 추천해주는 챗봇이야.\n",
    "아래는 사용자 질문과 관련된 혜택 문서들이야.\"\n",
    "\n",
    "# Context:\n",
    "{context}\n",
    "\n",
    "# Question:\n",
    "{question}\n",
    "\n",
    "# Answer:\n",
    "- 관련된 복지 혜택을 자연스럽고 친절하게 설명해줘\n",
    "- 대상 조건과 신청 방법도 간단히 알려줘\n",
    "- 혜택이 여러 개면 순서대로 정리해줘\n",
    "- 한글로, 부드럽고 공손한 말투로 작성해줘\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 트레이싱은 .env 설정만으로 자동 활성화됨\n",
    "# LANGSMITH_TRACING=true 설정 시 실행 로그를 LangSmith에서 확인 가능\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_sim = (\n",
    "    {\"context\": retriever_sim, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain_mmr = (\n",
    "    {\"context\": retriever_mmr, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 17:32:00.805 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\duffp\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-04-05 17:32:00.806 Session state does not function when running a script without `streamlit run`\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "\n",
    "# 제목\n",
    "st.title(\"혜택 추천 시스템: Similarity vs MMR\")\n",
    "\n",
    "# 사용자 질문 입력받기\n",
    "question = st.text_input(\"질문을 입력해주세요\", placeholder=\"예: 경기도에 거주하는 29살 남자인데 내가 받을 수 있는 혜택이 있을까?\")\n",
    "\n",
    "# 버튼 클릭 시 실행\n",
    "if st.button(\"혜택 추천 받기\"):\n",
    "    if question:\n",
    "        # 유사도 방식 응답\n",
    "        response_sim = chain_sim.invoke(question)\n",
    "        # MMR 방식 응답\n",
    "        response_mmr = chain_mmr.invoke(question)\n",
    "\n",
    "        # 출력\n",
    "        st.subheader(\"🔹 Similarity 방식 응답\")\n",
    "        st.write(response_sim)\n",
    "\n",
    "        st.subheader(\"🔸 MMR 방식 응답\")\n",
    "        st.write(response_mmr)\n",
    "    else:\n",
    "        st.warning(\"질문을 입력해주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook lanhchain.ipynb to script\n",
      "[NbConvertApp] Writing 5807 bytes to lanhchain.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script lanhchain.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
