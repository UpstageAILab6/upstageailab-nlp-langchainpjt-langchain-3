#!/usr/bin/env python
# coding: utf-8




# In[5]:


import json
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import PromptTemplate


# In[6]:


import os
from dotenv import load_dotenv
from langsmith import Client
from langchain_core.tracers import LangChainTracer

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸° ìƒíƒœ í™•ì¸
print("âœ… OpenAI í‚¤ ë¡œë“œë¨:", os.getenv("OPENAI_API_KEY") is not None)
print("âœ… LangSmith í‚¤ ë¡œë“œë¨:", os.getenv("LANGSMITH_API_KEY") is not None)

# LangSmith í™˜ê²½ ì„¤ì • (ë™ì  ì„¤ì •)
os.environ["LANGCHAIN_API_KEY"] = os.getenv("LANGSMITH_API_KEY")  # .envì—ì„œ ë¶ˆëŸ¬ì˜´
os.environ["LANGCHAIN_ENDPOINT"] = os.getenv("LANGSMITH_ENDPOINT") or "https://api.smith.langchain.com"
os.environ["LANGCHAIN_PROJECT"] = "Test"  # ì›í•˜ëŠ” í”„ë¡œì íŠ¸ ì´ë¦„

# LangSmith í´ë¼ì´ì–¸íŠ¸ ì§ì ‘ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŒ
client = Client()
print("í˜„ì¬ LangSmith í”„ë¡œì íŠ¸:", os.environ["LANGCHAIN_PROJECT"])


# In[7]:


# 1. ì ˆëŒ€ê²½ë¡œ ì§€ì •
absolute_path = r"C:\Users\duffp\RAG\upstageailab-nlp-langchainpjt-langchain-3\data\gov24_serviceList_all.json"

# 2. íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
if os.path.exists(absolute_path):
    print("âœ… íŒŒì¼ ê²½ë¡œ í™•ì¸ ì™„ë£Œ:", absolute_path)
else:
    print("âŒ ê²½ë¡œì— íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# 3. JSON ë¡œë“œ í•¨ìˆ˜ì— ì§ì ‘ ê²½ë¡œ ë„˜ê¸°ê¸°
def load_json_from_absolute_path(file_path: str):
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        print(f"ğŸ“¦ JSON ë¡œë“œ ì™„ë£Œ: í•­ëª© ìˆ˜ {len(data)}ê°œ")
        return data
    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return []

# ì‚¬ìš© ì˜ˆì‹œ
data = load_json_from_absolute_path(absolute_path)


# In[8]:


from langchain_core.documents import Document

documents = []
for item in data:
    content = f"""
ì„œë¹„ìŠ¤ëª…: {item.get('ì„œë¹„ìŠ¤ëª…')}
ì„œë¹„ìŠ¤ëª©ì : {item.get('ì„œë¹„ìŠ¤ëª©ì ìš”ì•½')}
ì§€ì›ëŒ€ìƒ: {item.get('ì§€ì›ëŒ€ìƒ')}
ì§€ì›ë‚´ìš©: {item.get('ì§€ì›ë‚´ìš©')}
ì‹ ì²­ë°©ë²•: {item.get('ì‹ ì²­ë°©ë²•')}
ì‹ ì²­ê¸°í•œ: {item.get('ì‹ ì²­ê¸°í•œ')}
ì„ ì •ê¸°ì¤€: {item.get('ì„ ì •ê¸°ì¤€')}
ì„œë¹„ìŠ¤ë¶„ì•¼: {item.get('ì„œë¹„ìŠ¤ë¶„ì•¼')}
ì†Œê´€ê¸°ê´€: {item.get('ì†Œê´€ê¸°ê´€ëª…')}
ë¬¸ì˜ì „í™”: {item.get('ì „í™”ë¬¸ì˜')}
ìƒì„¸ì¡°íšŒURL: {item.get('ìƒì„¸ì¡°íšŒURL')}
"""
    documents.append(Document(page_content=content.strip(), metadata={"ì„œë¹„ìŠ¤ID": item.get("ì„œë¹„ìŠ¤ID")}))

print(f"LangChain ë¬¸ì„œ ë³€í™˜ ì™„ë£Œ: {len(documents)}ê°œ")


# In[9]:


text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=50,
    separators=["\n\n", "\n", ".", " ", ""]
)


# In[10]:


split_documents = text_splitter.split_documents(documents)
print(f"ë¶„í• ëœ ë¬¸ì„œ ìˆ˜: {len(split_documents)}")
print("ì²« ì²­í¬ ë‚´ìš©:\n", split_documents[0].page_content[:500])


# In[15]:


for i, doc in enumerate(split_documents[:3]):
    print(f"\n--- ì²­í¬ {i+1} ---")
    print(doc.page_content)


# In[11]:


from langchain.vectorstores import FAISS
from langchain.embeddings.openai import OpenAIEmbeddings

# í† í° ì‚¬ìš© ì•ˆ í•˜ê³  ê¸°ì¡´ ë²¡í„°ë¥¼ ë¡œë“œí•¨
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.load_local("faiss_store", embeddings, allow_dangerous_deserialization=True)


# In[14]:


retriever_sim = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 10})
retriever_mmr = vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 10, "lambda_mult": 0.8})


# In[17]:


prompt = PromptTemplate.from_template("""ë„ˆëŠ” ë³µì§€ í˜œíƒì„ ì¶”ì²œí•´ì£¼ëŠ” ì±—ë´‡ì´ì•¼.
ì•„ë˜ëŠ” ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í˜œíƒ ë¬¸ì„œë“¤ì´ì•¼."

# Context:
{context}

# Question:
{question}

# Answer:
- ê´€ë ¨ëœ ë³µì§€ í˜œíƒì„ ìì—°ìŠ¤ëŸ½ê³  ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ì¤˜
- ëŒ€ìƒ ì¡°ê±´ê³¼ ì‹ ì²­ ë°©ë²•ë„ ê°„ë‹¨íˆ ì•Œë ¤ì¤˜
- í˜œíƒì´ ì—¬ëŸ¬ ê°œë©´ ìˆœì„œëŒ€ë¡œ ì •ë¦¬í•´ì¤˜
- í•œê¸€ë¡œ, ë¶€ë“œëŸ½ê³  ê³µì†í•œ ë§íˆ¬ë¡œ ì‘ì„±í•´ì¤˜
""")


# In[18]:


# LangSmith íŠ¸ë ˆì´ì‹±ì€ .env ì„¤ì •ë§Œìœ¼ë¡œ ìë™ í™œì„±í™”ë¨
# LANGSMITH_TRACING=true ì„¤ì • ì‹œ ì‹¤í–‰ ë¡œê·¸ë¥¼ LangSmithì—ì„œ í™•ì¸ ê°€ëŠ¥

llm = ChatOpenAI(model_name="gpt-4o", temperature=0)


# In[19]:


chain_sim = (
    {"context": retriever_sim, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

chain_mmr = (
    {"context": retriever_mmr, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)


# In[ ]:


import streamlit as st

# ì œëª©
st.title("í˜œíƒ ì¶”ì²œ ì‹œìŠ¤í…œ: Similarity vs MMR")

# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥ë°›ê¸°
question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”", placeholder="ì˜ˆ: ê²½ê¸°ë„ì— ê±°ì£¼í•˜ëŠ” 29ì‚´ ë‚¨ìì¸ë° ë‚´ê°€ ë°›ì„ ìˆ˜ ìˆëŠ” í˜œíƒì´ ìˆì„ê¹Œ?")

# ë²„íŠ¼ í´ë¦­ ì‹œ ì‹¤í–‰
if st.button("í˜œíƒ ì¶”ì²œ ë°›ê¸°"):
    if question:
        # ìœ ì‚¬ë„ ë°©ì‹ ì‘ë‹µ
        response_sim = chain_sim.invoke(question)
        # MMR ë°©ì‹ ì‘ë‹µ
        response_mmr = chain_mmr.invoke(question)

        # ì¶œë ¥
        st.subheader("ğŸ”¹ Similarity ë°©ì‹ ì‘ë‹µ")
        st.write(response_sim)

        st.subheader("ğŸ”¸ MMR ë°©ì‹ ì‘ë‹µ")
        st.write(response_mmr)
    else:
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")


# In[24]:


# In[ ]:




