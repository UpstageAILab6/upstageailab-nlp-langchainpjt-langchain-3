## 🧠 프로젝트 진행 중 고민 정리 및 선택 이유

---

### 📏 청크 분할 방식

#### 문제점
- 청크를 나누는 방식에서 **문자 길이 기반 vs 토큰 개수 기반** 중 선택해야 했음.

#### 해결 방안
- ✅ **토큰 개수 기반 분할 선택**
- **이유**:  
  LLM은 토큰 단위로 문장을 처리하고 이해하기 때문에 **문맥 단절을 최소화할 수 있는 토큰 기반 분할이 성능 면에서 더 적합**하다고 판단함.

---

### 💸 임베딩 비용 문제

#### 문제점 1
- 데이터 전처리를 생략하면 **중복 데이터가 많아져 임베딩 비용이 과도하게 증가**함.

#### 해결 방안
- ✅ **EDA를 통해 중복 제거**
- 전처리를 통해 완전 중복 및 부분 중복 데이터를 병합하여 데이터 수를 대폭 감소시킴.

##### 📐 중복 유형 및 처리 방식

| 구분 | 조건 | 처리 방식 | 통합 방식 상세 |
|------|------|------------|----------------|
| 1. 완전 중복 제거 | `서비스명`, `서비스ID`, 내용이 모두 동일 | 하나만 유지, 나머지 제거 | 완전히 동일한 항목 제거 |
| 2. ID/이름 같고 내용 다름 | `서비스명`, `서비스ID`는 같지만 세부 내용 다름 | 하나로 병합 | - 대표 ID 및 이름 유지<br>- `조건` 필드만 다르면 조건 병합<br>- 그 외 문자열: `||` 병합, dict: key 기준 병합 |
| 3. 이름 같고 ID 다름 | `서비스명` 동일, `서비스ID` 다름 | 하나로 병합 | - 대표 ID: 첫 항목 사용<br>- 문자열: `||` 병합<br>- 리스트: 중복 제거 후 병합<br>- dict: key 기준 병합<br>- 숫자 범위 (`대상연령` 등): min/max 병합 |

##### 📊 전처리 결과

| 단계     | 항목 수  |
|----------|----------|
| 병합 전  | 26,573개 |
| 병합 후  | 8,850개  |

---

#### 문제점 2
- 문서(정책)가 변경될 때마다 **전체 문서를 다시 임베딩**하는 것은 비효율적임.

#### 해결 방안
- ✅ **기존 JSON과 비교하여 변경된 문서만 선별 재임베딩**
- 📌 **기존 JSON 비교 방식 선택** (vs 해시 기반)
- **이유**:
  - 정부 서비스 데이터는 구조화된 JSON 형태로 주어짐
  - 각 서비스는 고유한 `서비스ID`로 식별 가능
  - 전체 JSON이 정기적으로 갱신되므로 구조화된 데이터를 비교하면 변경된 **필드**까지 추적 가능
  - 변경 감지뿐 아니라 **변경 이유 분석도 가능**
  - `prev.json` 백업은 `main_EDA.py`에서 자동 처리

##### 🔃 변경 유형 및 처리 방식

| 유형     | 조건                                                     | 처리 방식             |
|----------|----------------------------------------------------------|------------------------|
| 추가됨   | new에는 있고, prev에는 없음                              | 새로 임베딩 (`add`)   |
| 변경됨   | 두 JSON에 모두 존재하지만 내용이 다름                     | 재임베딩 (`update`)   |
| 삭제됨   | prev에는 있는데 new에는 없음                             | 벡터 DB에서 제거 (`delete`) |
