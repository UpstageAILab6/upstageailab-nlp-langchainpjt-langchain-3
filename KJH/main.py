import os
import json
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS

from data_pipeline.gov24_data_pipeline import run_gov24_data_pipeline
from modules.data_loader import detect_changes, convert_to_documents
from modules.chunk_splitter import split_by_char, split_by_token
from modules.upstage_embedding import UpstageEmbeddings
from modules.llm_prompt import make_prompt, query_solar
# path ì„¤ì •
BASE_DIR = os.path.dirname(os.path.abspath(__file__))   # c:\Users\jihu6\code\RAG\KJH 
DATA_PATH = os.path.join(BASE_DIR, "data", "combined_service_data_merged.json")
PREV_PATH = os.path.join(BASE_DIR, "data", "combined_service_data_merged_prev.json")

# í™˜ê²½ë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸°
env_path = os.path.join(BASE_DIR, ".env")
load_dotenv(dotenv_path=env_path)

GOV24_API_KEY=os.getenv("GOV24_API_KEY")
UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")
UPSTAGE_API_URL = os.getenv("UPSTAGE_API_URL")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
LANGSMITH_API_KEY = os.getenv("LANGSMITH_API_KEY")

# data_pipeline ì‹¤í–‰
run_gov24_data_pipeline()

# ğŸ”¹ JSON ë¡œë“œ (new)
with open(DATA_PATH, "r", encoding="utf-8") as f:
    new_data = json.load(f)
new_dict = {item["ì„œë¹„ìŠ¤ID"]: item for item in new_data}

# ğŸ”¹ JSON ë¡œë“œ (prev)
if os.path.exists(PREV_PATH):
    with open(PREV_PATH, "r", encoding="utf-8") as f:
        prev_data = json.load(f)
    prev_dict = {item["ì„œë¹„ìŠ¤ID"]: item for item in prev_data}
else:
    prev_data = []
    prev_dict = {}

# data ë³€í™˜ í™•ì¸
added, updated, deleted = detect_changes(new_data=new_data, prev_data=prev_data)
# ì„ë² ë”© ê°ì²´ ìƒì„±
embedding = UpstageEmbeddings(api_key=UPSTAGE_API_KEY,api_url=UPSTAGE_API_URL,batch_size=64)

if added or updated or deleted:
    print("ì„ë² ë”© ì—…ë°ì´íŠ¸ í•„ìš”\n")
    print(len(added),len(updated),len(deleted))
    documents = convert_to_documents(new_data)
    # ì²­í¬ ë¶„í• 
    token_split_docs = split_by_token(documents=documents)

    # FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥
    db = FAISS.from_documents(token_split_docs, embedding)  # ë¬¸ì„œ ì„ë² ë”© í›„ FAISS ì¸ë±ìŠ¤ ìƒì„±
    db.save_local("faiss_index")  # ì¸ë±ìŠ¤ë¥¼ ë¡œì»¬ ë””ë ‰í† ë¦¬ì— ì €ì¥
else:
    print("ìˆ˜ì •ì‚¬í•­ì´ ì—†ì–´ ì„ë² ë”© ê³¼ì •ì„ ê±´ë„ˆ ë›°ì—ˆìŠµë‹ˆë‹¤.")

# âœ… allow_dangerous_deserialization=True ì¶”ê°€
db = FAISS.load_local(
    "faiss_index",
    embeddings=embedding,
    allow_dangerous_deserialization=True  # âœ… Pickle ë¡œë“œ í—ˆìš©
)

# ì§ˆë¬¸ ì…ë ¥ ë°›ê¸°
query = input("ğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ")

# ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸°
docs = db.similarity_search(query, k=3)

2# ğŸ” ìœ ì‚¬ ë¬¸ì„œ + ì¡°ê±´ ì¶œë ¥
for i, doc in enumerate(docs):
    print(f"ğŸ“„ ë¬¸ì„œ {i+1}")
    # ğŸ”¸ ë¬¸ì„œ ìš”ì•½ (ì•ë¶€ë¶„ë§Œ ì¶œë ¥)
    print(doc.page_content[:500])
    # ğŸ”¸ ì¡°ê±´ ì •ë³´ ì¶œë ¥
    ì¡°ê±´ë“¤ = doc.metadata.get("ì¡°ê±´", {})
    ì„ íƒëœ_ì¡°ê±´ = [k for k, v in ì¡°ê±´ë“¤.items() if v]    
    print(f"ğŸ“Œ ì¡°ê±´ íƒœê·¸: {', '.join(ì„ íƒëœ_ì¡°ê±´) if ì„ íƒëœ_ì¡°ê±´ else 'ì¡°ê±´ ì—†ìŒ'}")
    print("-" * 50)

# prompt ë¶ˆëŸ¬ì˜¤ê¸°
prompt = make_prompt(retrieved_docs=docs, query=query)

# LLMì„ ì´ìš©í•´ ë¬¸ì„œ ì°¾ê¸°
print(query_solar(prompt=prompt, api_key=UPSTAGE_API_KEY))
