import os
import json
import streamlit as st
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS

from modules.data_loader import detect_changes, convert_to_documents
from modules.chunk_splitter import split_by_token
from modules.upstage_embedding import UpstageEmbeddings
from modules.llm_prompt import make_prompt, query_solar

# ğŸ“ ê²½ë¡œ ì„¤ì •
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_PATH = os.path.join(BASE_DIR, "data", "combined_service_data_merged.json")
PREV_PATH = os.path.join(BASE_DIR, "data", "combined_service_data_merged_prev.json")
INDEX_DIR = os.path.join(BASE_DIR, "faiss_index")
INDEX_FILE = os.path.join(INDEX_DIR, "index.faiss")

# ğŸ” í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv(os.path.join(BASE_DIR, ".env"))
UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")
UPSTAGE_API_URL = os.getenv("UPSTAGE_API_URL")

# ğŸŒ Streamlit UI
st.set_page_config(page_title="ì •ë¶€ì§€ì› ì„œë¹„ìŠ¤ ì§ˆë¬¸", page_icon="ğŸ“")
st.title("ğŸ“š ì •ë¶€ì§€ì› ì„œë¹„ìŠ¤ RAG ë°ëª¨")

# âœ… ë²¡í„° DB ì´ˆê¸°í™” (ìºì‹œ ì‚¬ìš©)
@st.cache_resource
def load_db():
    with open(DATA_PATH, "r", encoding="utf-8") as f:
        new_data = json.load(f)

    if os.path.exists(PREV_PATH):
        with open(PREV_PATH, "r", encoding="utf-8") as f:
            prev_data = json.load(f)
    else:
        prev_data = []

    added, updated, deleted = detect_changes(new_data=new_data, prev_data=prev_data)
    embedding = UpstageEmbeddings(api_key=UPSTAGE_API_KEY, api_url=UPSTAGE_API_URL)

    if added or updated or deleted or not os.path.exists(INDEX_FILE):
        documents = convert_to_documents(new_data)
        split_docs = split_by_token(documents)
        db = FAISS.from_documents(split_docs, embedding)
        db.save_local(INDEX_DIR)
    else:
        db = FAISS.load_local(INDEX_DIR, embeddings=embedding, allow_dangerous_deserialization=True)

    return db

# ğŸ”¹ ë²¡í„° DB ë¡œë”©
db = load_db()

# ğŸ” ì§ˆë¬¸ ì…ë ¥
query = st.text_input("ğŸ’¬ ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ: 25ì„¸ ì—¬ì„±ì´ ë°›ì„ ìˆ˜ ìˆëŠ” ì£¼ê±°ì§€ì›ì€?")
if query:
    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        docs = db.similarity_search(query, k=3)

        for i, doc in enumerate(docs):
            st.markdown(f"### ğŸ“„ ë¬¸ì„œ {i+1}")
            st.code(doc.page_content[:500])
            ì¡°ê±´ë“¤ = doc.metadata.get("ì¡°ê±´", {})
            ì„ íƒëœ_ì¡°ê±´ = [k for k, v in ì¡°ê±´ë“¤.items() if v]
            if ì„ íƒëœ_ì¡°ê±´:
                st.markdown("**ğŸ“Œ ì¡°ê±´ íƒœê·¸:** " + ", ".join(ì„ íƒëœ_ì¡°ê±´))

        prompt = make_prompt(retrieved_docs=docs, query=query)
        answer = query_solar(prompt=prompt, api_key=UPSTAGE_API_KEY)

        st.markdown("### ğŸ¤– ì‘ë‹µ")
        st.success(answer)
