import os  # ìš´ì˜ì²´ì œ ê´€ë ¨ ê¸°ëŠ¥ ì œê³µ (ê²½ë¡œ ë“±)
import urllib.parse  # URL ì¸ì½”ë”©ì„ ìœ„í•œ ëª¨ë“ˆ
import pandas as pd  # ë°ì´í„°í”„ë ˆì„ ì‚¬ìš©
import shutil  # íŒŒì¼ ë³µì‚¬ ë“±
from collections import defaultdict, Counter
from dotenv import load_dotenv  # .env íŒŒì¼ ë¡œë“œ
from gov24_api_fetcher import fetch_to_pd  # API ìš”ì²­ í•¨ìˆ˜
from support_model_crawler import crawl_support_conditions_model  # ì¡°ê±´ ì„¤ëª… í¬ë¡¤ë§ í•¨ìˆ˜

# ---------------------------
# ğŸ”§ 1. .env ë¡œë”© ë° API í‚¤ ì¸ì½”ë”©
# ---------------------------

base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))  # EDA ê¸°ì¤€ ìƒìœ„ ê²½ë¡œ (KJH)
env_path = os.path.join(base_dir, ".env")
load_dotenv(dotenv_path=env_path)

GOV24_API_KEY = os.getenv("GOV24_API_KEY")  # GOV24_API_KEY í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
if GOV24_API_KEY is None:  # í™˜ê²½ ë³€ìˆ˜ ì—†ì„ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬
    raise ValueError("âŒ .envì—ì„œ 'GOV24_API_KEY'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

encoded_key = urllib.parse.quote(GOV24_API_KEY, safe='')  # URL ì¸ì½”ë”©ëœ API í‚¤ ìƒì„±

# ---------------------------
# ğŸ“ 2. ê²½ë¡œ ì„¤ì •
# ---------------------------
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))  # EDA ê¸°ì¤€ ìƒìœ„ ê²½ë¡œ (KJH)
DATA_DIR = os.path.join(BASE_DIR, "data")  # ë°ì´í„° ì €ì¥ í´ë” ê²½ë¡œ
os.makedirs(DATA_DIR, exist_ok=True)  # ë°ì´í„° í´ë” ì—†ìœ¼ë©´ ìƒì„±

DETAIL_FILENAME = "serviceDetail_all"
CONDITIONS_FILENAME = "supportConditions_all"
MODEL_FILENAME = "supportConditions_model"



detail_json_path = os.path.join(DATA_DIR, f"{DETAIL_FILENAME}.json")
conditions_json_path = os.path.join(DATA_DIR, f"{CONDITIONS_FILENAME}.json")
model_json_path = os.path.join(DATA_DIR, f"{MODEL_FILENAME}.json")
combined_json_path = os.path.join(DATA_DIR, "combined_service_data.json")
merged_json_path = os.path.join(DATA_DIR, "combined_service_data_merged.json")

# ---------------------------
# ğŸ›Ÿ 3. ê¸°ì¡´ JSON ë°±ì—… í•¨ìˆ˜
# ---------------------------
def backup_if_exists(src_path: str, backup_name: str):
    if os.path.exists(src_path):
        backup_path = os.path.join(DATA_DIR, f"{backup_name}_prev.json")
        shutil.copy(src_path, backup_path)
        print(f"ğŸ“¦ ë°±ì—… ì™„ë£Œ: {backup_path}")

# ---------------------------
# ğŸ“¥ 4. ë°ì´í„° ìˆ˜ì§‘
# ---------------------------
detail_df = fetch_to_pd("https://api.odcloud.kr/api/gov24/v3/serviceDetail", encoded_key)
conditions_df = fetch_to_pd("https://api.odcloud.kr/api/gov24/v3/supportConditions", encoded_key)
model_df = crawl_support_conditions_model()

# ---------------------------
# ğŸ”¤ 5. í•­ëª©ì½”ë“œ â†’ ì„¤ëª… ë§¤í•‘ ë° ì¤‘ë³µ ì²˜ë¦¬
# ---------------------------
code_to_desc = dict(zip(model_df["í•­ëª©ì½”ë“œ"], model_df["ì„¤ëª…"]))

desc_counter = Counter()
new_columns = []
for col in conditions_df.columns:
    desc = code_to_desc.get(col, col)
    desc_counter[desc] += 1
    if desc_counter[desc] > 1:
        desc = f"{desc}_{desc_counter[desc]}"
    new_columns.append(desc)
conditions_df.columns = new_columns
print("âœ… supportConditions_df column names mapped to description")

# ---------------------------
# ğŸ§¹ 6. ì¤‘ë³µ ì„¤ëª… ì»¬ëŸ¼ í†µí•©
# ---------------------------
merged_df = pd.DataFrame(index=conditions_df.index)
column_groups = defaultdict(list)
for col in conditions_df.columns:
    base_name = col.split("_")[0]
    column_groups[base_name].append(col)

for base_name, col_list in column_groups.items():
    merged_series = conditions_df[col_list[0]]
    for other_col in col_list[1:]:
        merged_series = merged_series.combine_first(conditions_df[other_col])
    merged_df[base_name] = merged_series
conditions_df = merged_df
print("âœ… Duplicate description columns merged")

# ---------------------------
# ğŸ› ï¸ 7. ì¡°ê±´ í•„ë“œ ê°€ê³µ
# ---------------------------
exclude_condition_cols = ["ì„œë¹„ìŠ¤ëª…", "ëŒ€ìƒì—°ë ¹(ì‹œì‘)", "ëŒ€ìƒì—°ë ¹(ì¢…ë£Œ)"]
condition_cols = [col for col in conditions_df.columns if col not in exclude_condition_cols]

conditions_df["ì¡°ê±´"] = conditions_df.apply(
    lambda row: {
        col: True for col in condition_cols if str(row[col]).strip().upper() == "Y"
    },
    axis=1
)

conditions_df["ëŒ€ìƒì—°ë ¹"] = conditions_df.apply(
    lambda row: {
        "ì‹œì‘": int(row["ëŒ€ìƒì—°ë ¹(ì‹œì‘)"]) if pd.notnull(row["ëŒ€ìƒì—°ë ¹(ì‹œì‘)"]) else None,
        "ì¢…ë£Œ": int(row["ëŒ€ìƒì—°ë ¹(ì¢…ë£Œ)"]) if pd.notnull(row["ëŒ€ìƒì—°ë ¹(ì¢…ë£Œ)"]) else None,
    },
    axis=1
)

# ---------------------------
# ğŸ”— 8. detail_dfì™€ ë³‘í•©
# ---------------------------
combined_df = pd.merge(
    detail_df,
    conditions_df[["ì„œë¹„ìŠ¤ëª…", "ëŒ€ìƒì—°ë ¹", "ì¡°ê±´"]],
    on="ì„œë¹„ìŠ¤ëª…",
    how="inner"
)

# ---------------------------
# ğŸ“š 9. combined_dfì—ì„œ ì„œë¹„ìŠ¤ëª… ê¸°ì¤€ ë³‘í•© ì •ë³´ ì €ì¥
# ---------------------------
from copy import deepcopy

merged_rows = []
for name, group in combined_df.groupby("ì„œë¹„ìŠ¤ëª…"):
    base = deepcopy(group.iloc[0].to_dict())
    for _, row in group.iloc[1:].iterrows():
        for col, val in row.items():
            base_val = base.get(col)
            if isinstance(base_val, str) and isinstance(val, str) and base_val != val:
                base[col] = "||".join(sorted(set(base_val.split("||") + val.split("||"))))
            elif isinstance(base_val, list) and isinstance(val, list):
                base[col] = list(set(base_val) | set(val))
            elif isinstance(base_val, dict) and isinstance(val, dict):
                merged = {}
                for k in set(base_val) | set(val):
                    v1 = base_val.get(k)
                    v2 = val.get(k)
                    merged[k] = v1 if v1 is not None else v2
                base[col] = merged
            elif pd.isna(base_val) and pd.notna(val):
                base[col] = val
            elif base_val != val:
                base[col] = f"{base_val}||{val}"
    merged_rows.append(base)

combined_merged_df = pd.DataFrame(merged_rows)


# ---------------------------
# ğŸ’¾ 10. ì €ì¥ ì „ ë°±ì—…
# ---------------------------
backup_if_exists(detail_json_path, DETAIL_FILENAME)
backup_if_exists(conditions_json_path, CONDITIONS_FILENAME)
backup_if_exists(model_json_path, MODEL_FILENAME)
backup_if_exists(combined_json_path, "combined_service_data")
backup_if_exists(merged_json_path, "combined_service_data_merged")

# ---------------------------
# ğŸ“¤ 11. JSON ì €ì¥
# ---------------------------
detail_df.to_json(detail_json_path, orient="records", force_ascii=False, indent=2)
conditions_df.to_json(conditions_json_path, orient="records", force_ascii=False, indent=2)
model_df.to_json(model_json_path, orient="records", force_ascii=False, indent=2)
combined_df.to_json(combined_json_path, orient="records", force_ascii=False, indent=2)
combined_merged_df.to_json(merged_json_path, orient="records", force_ascii=False, indent=2)


print(f"âœ… JSON ì €ì¥ ì™„ë£Œ: {detail_json_path}")
print(f"âœ… JSON ì €ì¥ ì™„ë£Œ: {conditions_json_path}")
print(f"âœ… JSON ì €ì¥ ì™„ë£Œ: {model_json_path}")
print(f"âœ… ë³‘í•© JSON ì €ì¥ ì™„ë£Œ: {combined_json_path}")
print(f"âœ… ë³‘í•©ëœ combined_df ì €ì¥ ì™„ë£Œ: {merged_json_path}")
print(f"ğŸ” ë³‘í•© ì „: {len(combined_df)} / ë³‘í•© í›„: {len(combined_merged_df)}")